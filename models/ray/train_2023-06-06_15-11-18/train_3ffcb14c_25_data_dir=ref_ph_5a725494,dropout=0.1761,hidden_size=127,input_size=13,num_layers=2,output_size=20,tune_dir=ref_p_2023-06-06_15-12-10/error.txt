Failure # 1 (occurred at 2023-06-06_15-12-38)
Traceback (most recent call last):
  File "/home/vscode/.cache/pypoetry/virtualenvs/deep-learning-G3c9zDRR-py3.9/lib/python3.9/site-packages/ray/tune/execution/ray_trial_executor.py", line 1231, in get_next_executor_event
    future_result = ray.get(ready_future)
  File "/home/vscode/.cache/pypoetry/virtualenvs/deep-learning-G3c9zDRR-py3.9/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/home/vscode/.cache/pypoetry/virtualenvs/deep-learning-G3c9zDRR-py3.9/lib/python3.9/site-packages/ray/_private/worker.py", line 2523, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.17.0.3, ID: 16e74e387ef6db5b920e4b3e5f582347b303286e2d29278b6aef8f52) where the task (actor ID: 020dc7c06ad5ff18f919856801000000, name=ImplicitFunc.__init__, pid=30790, memory used=0.35GB) was running was 7.07GB / 7.44GB (0.950163), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 95fe98edd32d0077bf157f58c93a43bfccad2e95be2f3e4450a4a6bb) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.0.3`. To see the logs of the worker, use `ray logs worker-95fe98edd32d0077bf157f58c93a43bfccad2e95be2f3e4450a4a6bb*out -ip 172.17.0.3. Top 10 memory users:
PID	MEM(GB)	COMMAND
1665	0.64	/vscode/vscode-server/bin/linux-x64/b3e4e68a0bc097f0ae7907b217c1119af9e03435/node /home/vscode/.vsco...
30776	0.38	ray::ImplicitFunc.train
30780	0.38	ray::ImplicitFunc.train
30629	0.37	ray::ImplicitFunc.train
30778	0.37	ray::ImplicitFunc.train
30773	0.37	ray::ImplicitFunc.train
30788	0.35	ray::ImplicitFunc.train
30781	0.35	ray::ImplicitFunc.train
30785	0.35	ray::ImplicitFunc.train
30790	0.35	ray::ImplicitFunc.train
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

